{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc52829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Normilisation Complete\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from numpy import std, mean\n",
    "import statistics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "\n",
    "#Import File\n",
    "global all_df\n",
    "all_df=pd.read_csv('TrainDataset2023.csv', index_col=False) #Read from File\n",
    "all_df.drop('ID', axis=1, inplace=True) # Drop ID - not needed for training\n",
    "\n",
    "#Impute Missing Values\n",
    "imputer = SimpleImputer(missing_values = 999, strategy=\"median\") # Test Other Methods\n",
    "SimpleImputer(missing_values = 999)\n",
    "s = 0\n",
    "for i in all_df:\n",
    "    imputer.fit(all_df)\n",
    "    array = np.array(all_df[i])\n",
    "    all_df[i] = imputer.fit_transform(array.reshape(-1, 1))       \n",
    "    \n",
    "#Normalise Values\n",
    "\n",
    "#Min Max \n",
    "colno = 0\n",
    "for col in all_df:\n",
    "    if col == 'RelapseFreeSurvival (outcome)':\n",
    "        continue\n",
    "    if colno >= 12: # ONLY NORMALISE MRI SCAN DATA\n",
    "        colmean = np.median(all_df[col])\n",
    "        colstd = np.std(all_df[col])\n",
    "        upper = colmean + (3*colstd)\n",
    "        lower = colmean - (3*colstd) # USING MIN ALSO SEEMS TO SKEW DATA\n",
    "        #Comment out to cancel\n",
    "        #all_df[col] = minmax_scale(all_df[col], feature_range=(lower,upper)) #Minimal Change - Downscales severity of Mean Squared Error\n",
    "    colno+=1\n",
    "        \n",
    "#Z Score\n",
    "#scaler = StandardScaler()\n",
    "#for col in all_df:\n",
    "#    scaler.fit(all_df[[col]])\n",
    "#    scaler.mean_\n",
    "#    all_df[col] = scaler.transform(all_df[[col]]) # Still seeing minimal change\n",
    "    \n",
    "#Things to Consider\n",
    "# Curse of Dimensionality - We have fewer data points, but a large selection of features available\n",
    "    \n",
    "# Data Normilisation Methods - To Do/Consider\n",
    "# 1) Z-Normilisation - DONE - Trying to avoid scaling\n",
    "# 2) Min-Max Normilisation - DONE - using STD to avoid major changes (most outliers are acceptable)\n",
    "# 3) Vector Normilisation - Ad Hoc\n",
    "    \n",
    "#Feature Selection Methods\n",
    "# 1) Forward Feature Selection - Done\n",
    "# 2) LASSO - Regression Method - Done\n",
    "# 3) Chi Square - Categorical\n",
    "# 4) T-test - Categorical\n",
    "# 5) ANOVA - Categorical\n",
    "# 6) Recursive Feature Elimination - Very Expensive - Not loading in reasonable period (Output Not Tested)\n",
    "# 7) Sequential Feature Selection\n",
    "# 8) Ridge/ElasticNet\n",
    "# 9) Correlation/Variance\n",
    "\n",
    "#Dimensionality Reduction Methods\n",
    "# 1) PCA (Principle Component Analysis)\n",
    "# 2) LDA (Linear Discriminant Analysis)\n",
    "\n",
    "# Regression Methods\n",
    "# 1) Linear Regression - Done (cleanup)\n",
    "# 2) SVM - Done\n",
    "# 3) Decision Tree - Done\n",
    "# 4) Random Forest - Done (needs refining)\n",
    "# 5) ANN - Done\n",
    "\n",
    "# Evaluation Methods\n",
    "# 1) K-Fold\n",
    "print('Data Normilisation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2065e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Start \n",
      "\n",
      "Best Features = ['original_glcm_SumAverage', 'original_shape_MajorAxisLength', 'HER2', 'original_glszm_ZonePercentage', 'original_shape_Sphericity', 'original_glcm_JointAverage']\n",
      "Number of Features Used = 6\n",
      "\n",
      "46.23166846375699\n",
      "16.736654682423246\n",
      "\n",
      "24.659063772843915\n",
      "22.17221399757321\n",
      "\n",
      "13.223831686633982\n",
      "25.064825822546478\n",
      "\n",
      "15.959133827991419\n",
      "24.622926417576362\n",
      "\n",
      "25.471112265739453\n",
      "22.818609835404946\n",
      "\n",
      "Validation Set: \n",
      "The Avg. Accuracy of Linear Regression is -1.23\n",
      "The Avg. Mean Square Error for Linear Regression Test is 25.1090\n",
      "The Avg. Mean Square Error for Linear Regression Train is 22.2830\n",
      "\n",
      "Best Features = ['original_glcm_SumAverage']\n",
      "Number of Features Used = 1\n",
      "\n",
      "43.68102060542206\n",
      "15.959511261456308\n",
      "\n",
      "21.099174101272165\n",
      "21.199772212011865\n",
      "\n",
      "11.680622629403643\n",
      "23.710283670850618\n",
      "\n",
      "24.408664290152206\n",
      "21.061809221773466\n",
      "\n",
      "13.360499999966828\n",
      "23.15928108300813\n",
      "\n",
      "\n",
      "No Validation Set: \n",
      "The Avg. Accuracy of Linear Regression is -0.84\n",
      "The Avg. Mean Square Error for Linear Regression Test is 22.8460\n",
      "The Avg. Mean Square Error for Linear Regression Train is 43.3012\n",
      "\n",
      "SFS Best No. of Features = 2\n",
      "['original_shape_Sphericity' 'original_firstorder_Uniformity']\n",
      "43.00553137440749\n",
      "15.69892560034042\n",
      "\n",
      "20.365856314199338\n",
      "20.815038545380627\n",
      "\n",
      "11.478044550506757\n",
      "23.161345285071068\n",
      "\n",
      "22.86715406562639\n",
      "20.72451996539345\n",
      "\n",
      "14.154976335507836\n",
      "22.439552883098855\n",
      "\n",
      "\n",
      "SFS Set: \n",
      "The Avg. Accuracy of Linear Regression is -0.74\n",
      "The Avg. Mean Square Error for Linear Regression Test is 22.3743\n",
      "The Avg. Mean Square Error for Linear Regression Train is 20.5679\n",
      "\n",
      "Linear Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression - High MSE by default\n",
    "print('Linear Regression Start \\n')\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "#Forward Feature Selection\n",
    "# LIMIT HOW MANY FEATURES CAN BE USED?\n",
    "def FFS(trainx, trainy, validationx, validationy, maxscore):\n",
    "    F = trainx.columns.tolist()\n",
    "    #F.remove('RelapseFreeSurvival (outcome)')\n",
    "    X = []\n",
    "    B = maxscore\n",
    "    while X != F:\n",
    "        Y = []\n",
    "        remfeatures = list(set(F)-set(Y))\n",
    "        for i in remfeatures:\n",
    "            temp = Y+[i]\n",
    "            lin_reg.fit(trainx[temp],trainy)\n",
    "            lin_reg.intercept_,lin_reg.coef_\n",
    "            y_pred = lin_reg.predict(validationx[temp])\n",
    "            mse = mean_absolute_error(validationy, y_pred)\n",
    "            if mse <= B-(B/40): #Fine Tune the Optimal Increase for a new feature to be worthwhile\n",
    "                B = mean_absolute_error(validationy, y_pred)\n",
    "                Y = temp\n",
    "                #print(\"Features Updated: \"+str(Y))\n",
    "                #print(\"Best Score Updated: \"+str(B))\n",
    "        if X != [] and mean_absolute_error(validationy, y_pred) < B:\n",
    "            #print(\"Best X Features = \"+str(X))\n",
    "            #print(\"Best Features Accuracy = \"+str(B))\n",
    "            break\n",
    "        else:\n",
    "            X = Y\n",
    "            break\n",
    "    print(\"Best Features = \"+str(X))\n",
    "    print('Number of Features Used = '+str(len(X)))\n",
    "    #y_pred = lin_reg.predict(validationx[X])\n",
    "    #print(\"Feature Mean Squared Error = \"+str(mean_squared_error(validationy, y_pred)))\n",
    "    print(\"\")\n",
    "    return X\n",
    "\n",
    "def FFS2(xset, yset, maxscore):\n",
    "    F = xset.columns.tolist()\n",
    "    #F.remove('RelapseFreeSurvival (outcome)')\n",
    "    X = []\n",
    "    B = maxscore\n",
    "    while X != F: #Add Limit on how many features can be used\n",
    "        Y = []\n",
    "        remfeatures = list(set(F)-set(Y))\n",
    "        for i in remfeatures:\n",
    "            temp = Y+[i]\n",
    "            lin_reg.fit(xset[temp],yset)\n",
    "            lin_reg.intercept_,lin_reg.coef_\n",
    "            y_pred = lin_reg.predict(xset[temp])\n",
    "            mse = mean_absolute_error(yset, y_pred)\n",
    "            if mse <= B-(B/40):\n",
    "                B = mean_absolute_error(yset, y_pred)\n",
    "                Y = temp\n",
    "                #print(\"Features Updated: \"+str(Y))\n",
    "                #print(\"Best Score Updated: \"+str(B))\n",
    "        if X != [] and mean_absolute_error(yset, y_pred) < B:\n",
    "            #print(\"Best X Features = \"+str(X))\n",
    "            #print(\"Best Features Accuracy = \"+str(B))\n",
    "            break\n",
    "        else:\n",
    "            X = Y\n",
    "            break\n",
    "    print(\"Best Features = \"+str(X))\n",
    "    print('Number of Features Used = '+str(len(X)))\n",
    "    #y_pred = lin_reg.predict(validationx[X])\n",
    "    #print(\"Feature Mean Squared Error = \"+str(mean_squared_error(validationy, y_pred)))\n",
    "    print(\"\")\n",
    "    return X\n",
    "\n",
    "# Add validation Set for testing FFS\n",
    "\n",
    "X = FFS(train_X, train_y, validate_X, validate_y, 1000) #Test FFS\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X[X]) # Change when adding validation set\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    lin_reg.fit(Xs[train],y[train])\n",
    "    lin_reg.intercept_,lin_reg.coef_\n",
    "    regression_score = lin_reg.score(Xs[test], y[test])\n",
    "    y_pred = lin_reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = lin_reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print(mse)\n",
    "    print(mse2)\n",
    "    print(\"\")\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "print('Validation Set: ')\n",
    "print('The Avg. Accuracy of Linear Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Linear Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Linear Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print(\"\")\n",
    "\n",
    "X = FFS2(x, y, 1000) #Test FFS\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(all_df[X]) # Change when adding validation set\n",
    "mse_total = 0\n",
    "mse2_total\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    lin_reg.fit(Xs[train],y[train])\n",
    "    lin_reg.intercept_,lin_reg.coef_\n",
    "    regression_score = lin_reg.score(Xs[test], y[test])\n",
    "    y_pred = lin_reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = lin_reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print(mse)\n",
    "    print(mse2)\n",
    "    print(\"\")\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "print('\\nNo Validation Set: ')\n",
    "print('The Avg. Accuracy of Linear Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Linear Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Linear Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('')\n",
    "\n",
    "#Create loop to determine best no of features\n",
    "featureno = 1\n",
    "score = 10000\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "while featureno < 50:\n",
    "    featuretest = []\n",
    "    sfs = SequentialFeatureSelector(lin_reg, n_features_to_select=featureno)\n",
    "    sfs.fit(x,y)\n",
    "    feats = sfs.get_feature_names_out() \n",
    "    for i in feats:\n",
    "        featuretest.append(i)\n",
    "    x2 = all_df[featuretest] \n",
    "    lin_reg.fit(x2,y)\n",
    "    lin_reg.intercept_,lin_reg.coef_\n",
    "    y_pred = lin_reg.predict(x2)\n",
    "    mse = mean_absolute_error(y, y_pred)\n",
    "    if mse < score-(score/40):\n",
    "        featureno+=1\n",
    "        score = mse\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "print(\"SFS Best No. of Features = \"+str(featureno))\n",
    "    \n",
    "sfs = SequentialFeatureSelector(lin_reg, n_features_to_select=featureno)\n",
    "sfs.fit(x,y)\n",
    "feats = sfs.get_feature_names_out() \n",
    "print(feats)\n",
    "feature = []\n",
    "for i in feats:\n",
    "    feature.append(i)\n",
    "\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df[feature]\n",
    "Xs = scaler.fit_transform(x) \n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    lin_reg.fit(Xs[train],y[train])\n",
    "    lin_reg.intercept_,lin_reg.coef_\n",
    "    regression_score = lin_reg.score(Xs[test], y[test])\n",
    "    y_pred = lin_reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = lin_reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print(mse)\n",
    "    print(mse2)\n",
    "    print(\"\")\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "print('\\nSFS Set: ')\n",
    "print('The Avg. Accuracy of Linear Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Linear Regression Test is {:03.4f}'.format(mse_total/5))  \n",
    "print('The Avg. Mean Square Error for Linear Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "\n",
    "print('\\nLinear Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60f8bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Regression Start \n",
      "\n",
      "SVM Best Feature = original_glszm_ZoneEntropy\n",
      "['original_shape_Sphericity']\n",
      "2695.5080701112984\n",
      "880.8462296610608\n",
      "321.7956102827304\n",
      "496.97687552784896\n",
      "564.5021345309962\n",
      "The Avg. Accuracy of SVM Regression is -1.08\n",
      "The Avg. Mean Square Error for SVM Regression is 991.9258\n",
      "\n",
      "SVM Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Regressor Start\n",
    "print('\\nSVM Regression Start \\n')\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "#Forward Feature Selection for 1 feature\n",
    "F = all_df.columns.tolist()\n",
    "F.remove('RelapseFreeSurvival (outcome)')\n",
    "B = 1000\n",
    "Y = []\n",
    "X = []\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "columnno = 0\n",
    "for i in F:\n",
    "    X = train_X[[i]].values\n",
    "    regressor.fit(X, train_y.values)\n",
    "    y_pred = regressor.predict(validate_X[[i]])\n",
    "    mse = mean_squared_error(validate_y, y_pred)\n",
    "    if mse < B:\n",
    "        Y = i\n",
    "        B = mse\n",
    "        #print(\"Best Feature Updated: \"+str(i))\n",
    "        #print(\"Best Accuracy Updated: \"+str(B))\n",
    "\n",
    "print('SVM Best Feature = '+str(Y))   \n",
    "\n",
    "#Feature Elimination\n",
    "#regressor2 = SVR(kernel = 'linear')\n",
    "#selector = RFE(regressor2, n_features_to_select=1)\n",
    "#selector = selector.fit(x,y)\n",
    "#print(selector.ranking_)\n",
    "#print('RFE = '+str(selector.estimator_))\n",
    "\n",
    "\n",
    "#Sequential Feature Selection\n",
    "#sfs = SequentialFeatureSelector(regressor, n_features_to_select=1)\n",
    "#sfs.fit(x,y)\n",
    "#feats = sfs.get_feature_names_out() \n",
    "#print(feats)\n",
    "\n",
    "\n",
    "\n",
    "#Set Best Feature\n",
    "x = train_X[[Y]].values\n",
    "\n",
    "#tmp x for SFS\n",
    "#x = train_X[[feats[0]]].values\n",
    "\n",
    "Xs = scaler.fit_transform(x)\n",
    "\n",
    "#K-fold\n",
    "mse_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    regressor.fit(Xs[train],y[train])\n",
    "    regression_score = regressor.score(Xs[test], y[test])\n",
    "    y_pred = regressor.predict(Xs[test])\n",
    "    mse = mean_squared_error(y[test], y_pred)\n",
    "    print(mse)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "\n",
    "print('The Avg. Accuracy of SVM Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for SVM Regression is {:03.4f}'.format(mse_total/5))\n",
    "print('\\nSVM Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a1b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regression Start \n",
      "\n",
      "Best Depth = 1\n",
      "2545.027346355209\n",
      "847.9014682274044\n",
      "367.4413178292528\n",
      "497.69985487520887\n",
      "651.1109299801504\n",
      "The Avg. Accuracy of Decision Tree Regression is -1.19\n",
      "The Avg. Mean Square Error for Decision Tree Regression is 981.8362\n",
      "\n",
      "Decision Tree Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression Start\n",
    "print('\\nDecision Tree Regression Start \\n')\n",
    "\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(x)\n",
    "\n",
    "depth = 1\n",
    "top_score = 10000\n",
    "best_depth = 1\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "while depth <= 50:\n",
    "    tree_clf = DecisionTreeRegressor(max_depth=depth)\n",
    "    tree_clf.fit(train_X, train_y) #Train Model\n",
    "    tree_clf.predict(train_X)\n",
    "    new_score = tree_clf.score(validate_X, validate_y)\n",
    "    y_pred = tree_clf.predict(validate_X)\n",
    "    mse = mean_squared_error(validate_y, y_pred)\n",
    "    #print(\"Depth = \"+str(depth)+\", MSE = \"+str(mse))\n",
    "    if mse <= top_score:\n",
    "        best_depth = depth\n",
    "        top_score = mse\n",
    "        #print(\"Depth = \"+str(depth)+\", Score = \"+str(new_score))\n",
    "    depth += 1\n",
    "    \n",
    "print(\"Best Depth = \"+str(best_depth))\n",
    "\n",
    "#K-fold\n",
    "mse_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "tree_clf = DecisionTreeRegressor(max_depth=best_depth)\n",
    "for train, test in kf.split(train_X, train_y):\n",
    "    tree_clf.fit(Xs[train],y[train])\n",
    "    regression_score = tree_clf.score(Xs[test], y[test])\n",
    "    y_pred = tree_clf.predict(Xs[test])\n",
    "    mse = mean_squared_error(y[test], y_pred)\n",
    "    print(mse)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "\n",
    "print('The Avg. Accuracy of Decision Tree Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Decision Tree Regression is {:03.4f}'.format(mse_total/5))\n",
    "print('\\nDecision Tree Regression End \\n')\n",
    "# Decision Tree Regression End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21f3c30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Regression Start \n",
      "\n",
      "2235.0422657305176\n",
      "3\n",
      "585.7467784979947\n",
      "3\n",
      "518.759685845668\n",
      "3\n",
      "623.0502159359238\n",
      "3\n",
      "616.4603620388452\n",
      "3\n",
      "The Avg. Accuracy of Neural Network Regression is -1.15\n",
      "The Avg. Mean Square Error for Neural Network Regression is 915.8119\n",
      "\n",
      "Neural Network Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neural Network Regression Start\n",
    "print('\\nNeural Network Regression Start \\n')\n",
    "\n",
    "X = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "#K-fold\n",
    "mse_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    mlp_clf = MLPRegressor(random_state=1, max_iter=300).fit(Xs[train], y[train])\n",
    "    regression_score = mlp_clf.score(Xs[test], y[test])\n",
    "    y_pred = mlp_clf.predict(Xs[test])\n",
    "    mse = mean_squared_error(y[test], y_pred)\n",
    "    print(mse)\n",
    "    print(mlp_clf.n_layers_)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "\n",
    "print('The Avg. Accuracy of Neural Network Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Neural Network Regression is {:03.4f}'.format(mse_total/5))\n",
    "print('\\nNeural Network Regression End \\n')\n",
    "# Neural Network Regression End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8256a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regression Start \n",
      "\n",
      "1\n",
      "2309.7383068216604\n",
      "2\n",
      "634.0435609015101\n",
      "2\n",
      "341.0219651727146\n",
      "2\n",
      "670.1780929000224\n",
      "2\n",
      "295.72777123917916\n",
      "The Avg. Mean Square Error for Random Forest Regression is 850.1419\n",
      "\n",
      "Random Forest Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regression Start\n",
    "print('\\nRandom Forest Regression Start \\n')\n",
    "\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "\n",
    "#Find best feature to accuracy ratio - random forest has built in feature selection via feature ranking\n",
    "# return Highest rated features\n",
    "def RFDepthSelection(trainx, trainy, testx, testy):\n",
    "    feature_limit = 1\n",
    "    mse_score = 1000\n",
    "    while feature_limit < 50:\n",
    "        rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = feature_limit)\n",
    "        rfregressor.fit(trainx, trainy)\n",
    "        y_pred = rfregressor.predict(testx)\n",
    "        mse = mean_squared_error(testy, y_pred)\n",
    "        if mse < mse_score - (mse_score/20):\n",
    "            mse_score = mse\n",
    "            feature_limit+=1\n",
    "            continue\n",
    "        else:\n",
    "            return feature_limit\n",
    "\n",
    "#Make x the validated feature\n",
    "#rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = feature_limit) #Default Measure = MSE\n",
    "#K-fold\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    print(featureno)\n",
    "    rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = featureno)\n",
    "    rfregressor.fit(Xs[train], y[train])\n",
    "    y_pred = rfregressor.predict(Xs[test])\n",
    "    mse = mean_squared_error(y[test], y_pred)\n",
    "    print(mse)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression is {:03.4f}'.format(mse_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')\n",
    "\n",
    "#Random Forest Regression End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81309af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LASSO Linear Regression Start \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLASSO Linear Regression Start \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mall_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelapseFreeSurvival (outcome)\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m all_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelapseFreeSurvival (outcome)\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mLasso()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\nLASSO Linear Regression Start \\n')\n",
    "\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "clf = linear_model.Lasso()\n",
    "\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    clf.fit(Xs[train],y[train])\n",
    "    y_pred = clf.predict(Xs[test])\n",
    "    mse = mean_squared_error(y[test], y_pred)\n",
    "    print(mse)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for LASSO Regression is {:03.4f}'.format(mse_total/5))\n",
    "print('\\nLASSO Linear Regression End \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
