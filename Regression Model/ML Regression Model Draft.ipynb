{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fd6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Normilisation Complete\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from numpy import std, mean\n",
    "import statistics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "\n",
    "#Import File\n",
    "global all_df\n",
    "all_df=pd.read_csv('TrainDataset2023.csv', index_col=False) #Read from File\n",
    "all_df.drop('ID', axis=1, inplace=True) # Drop ID - not needed for training\n",
    "\n",
    "#Impute Missing Values\n",
    "imputer = SimpleImputer(missing_values = 999, strategy=\"median\") # Test Other Methods\n",
    "SimpleImputer(missing_values = 999)\n",
    "s = 0\n",
    "for i in all_df:\n",
    "    imputer.fit(all_df)\n",
    "    array = np.array(all_df[i])\n",
    "    all_df[i] = imputer.fit_transform(array.reshape(-1, 1))       \n",
    "    \n",
    "#Normalise Values\n",
    "\n",
    "#Min Max \n",
    "colno = 0\n",
    "for col in all_df:\n",
    "    if col == 'RelapseFreeSurvival (outcome)':\n",
    "        continue\n",
    "    if colno >= 12: # ONLY NORMALISE MRI SCAN DATA\n",
    "        colmean = np.median(all_df[col])\n",
    "        colstd = np.std(all_df[col])\n",
    "        upper = colmean + (3*colstd)\n",
    "        lower = colmean - (3*colstd) # USING MIN ALSO SEEMS TO SKEW DATA\n",
    "        #Comment out to cancel\n",
    "        #all_df[col] = minmax_scale(all_df[col], feature_range=(lower,upper)) #Minimal Change - Downscales severity of Mean Squared Error\n",
    "    colno+=1\n",
    "    \n",
    "print('Data Normilisation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa25204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Regression Start \\n')\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "regressor = SVR(kernel = 'sigmoid')\n",
    "\n",
    "def FFS(trainx, trainy, validationx, validationy, maxscore):\n",
    "    F = trainx.columns.tolist()\n",
    "    #F.remove('RelapseFreeSurvival (outcome)')\n",
    "    X = []\n",
    "    B = maxscore\n",
    "    while X != F:\n",
    "        Y = []\n",
    "        remfeatures = list(set(F)-set(Y))\n",
    "        for i in remfeatures:\n",
    "            temp = Y+[i]\n",
    "            regressor.fit(trainx[temp],trainy)\n",
    "            y_pred = regressor.predict(validationx[temp])\n",
    "            mse = mean_absolute_error(validationy, y_pred)\n",
    "            if mse < B:#-(B/80): #Fine Tune the Optimal Increase for a new feature to be worthwhile\n",
    "                B = mean_absolute_error(validationy, y_pred)\n",
    "                Y = temp\n",
    "                #print(\"Features Updated: \"+str(Y))\n",
    "                #print(\"Best Score Updated: \"+str(B))\n",
    "        if X != [] and mean_absolute_error(validationy, y_pred) < B:\n",
    "            #print(\"Best X Features = \"+str(X))\n",
    "            #print(\"Best Features Accuracy = \"+str(B))\n",
    "            break\n",
    "        else:\n",
    "            X = Y\n",
    "            break\n",
    "    print(\"Best Features = \"+str(X))\n",
    "    print('Number of Features Used = '+str(len(X)))\n",
    "    #y_pred = lin_reg.predict(validationx[X])\n",
    "    #print(\"Feature Mean Squared Error = \"+str(mean_squared_error(validationy, y_pred)))\n",
    "    print(\"\")\n",
    "    return X\n",
    "        \n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "X = FFS(train_X, train_y, validate_X, validate_y, 1000) #Test FFS\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X[X])\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    regressor.fit(Xs[train],y[train])\n",
    "    regression_score = regressor.score(Xs[test], y[test])\n",
    "    y_pred = regressor.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = regressor.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "    print('')\n",
    "\n",
    "print('Validation Set: ')\n",
    "print('The Avg. Accuracy of Linear Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Absolute Error for Linear Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Absolute Error for Linear Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd394127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Regression Start \\n')\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "regressor = SVR(kernel = 'sigmoid')\n",
    "\n",
    "featureno = 1\n",
    "score = 10000\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "while featureno < 50:\n",
    "    featuretest = []\n",
    "    sfs = SequentialFeatureSelector(regressor, n_features_to_select=featureno)\n",
    "    sfs.fit(x,y)\n",
    "    feats = sfs.get_feature_names_out() \n",
    "    for i in feats:\n",
    "        featuretest.append(i)\n",
    "    x2 = train_X[featuretest] \n",
    "    regressor.fit(x2,train_y)\n",
    "    y_pred = regressor.predict(validate_X[featuretest])\n",
    "    mse = mean_absolute_error(validate_y, y_pred)\n",
    "    if mse < score:#-(score/20):\n",
    "        featureno+=1\n",
    "        score = mse\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "print(\"SFS Best No. of Features = \"+str(featureno))\n",
    "    \n",
    "sfs = SequentialFeatureSelector(regressor, n_features_to_select=featureno)\n",
    "sfs.fit(x,y)\n",
    "feats = sfs.get_feature_names_out() \n",
    "print(feats)\n",
    "feature = []\n",
    "for i in feats:\n",
    "    feature.append(i)\n",
    "\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "x = train_X[feature]\n",
    "Xs = scaler.fit_transform(x) \n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    regressor.fit(Xs[train],y[train])\n",
    "    regression_score = regressor.score(Xs[test], y[test])\n",
    "    y_pred = regressor.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = regressor.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "    print('')\n",
    "    \n",
    "print('Validation Set: ')\n",
    "print('The Avg. Accuracy of Linear Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Absolute Error for Linear Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Absolute Error for Linear Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5717b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nDecision Tree Regression Start \\n')\n",
    "\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(x)\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "def RFDepthSelection(trainx, trainy, testx, testy):\n",
    "    feature_limit = 1\n",
    "    mse_score_feature = 1000\n",
    "    mse_score_depth = 1000\n",
    "    while feature_limit <= 114:\n",
    "        print('Feature Loop '+str(feature_limit))\n",
    "        mse_score_depth = 1000\n",
    "        depth_limit = 1\n",
    "        while depth_limit <= 114:\n",
    "            print('Depth Loop '+str(depth_limit))\n",
    "            rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, criterion = 'absolute_error',\n",
    "                                                max_features = feature_limit, max_depth = depth_limit)\n",
    "            rfregressor.fit(trainx, trainy)\n",
    "            y_pred = rfregressor.predict(testx)\n",
    "            mse = mean_absolute_error(testy, y_pred)\n",
    "            if mse < mse_score_depth:# - (mse_score/20):\n",
    "                mse_score_depth = mse\n",
    "                depth_limit+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        print('CHECK MSE')\n",
    "        print('MSE = '+str(mse))\n",
    "        print('MSE_SCORE Feature = '+str(mse_score_feature))\n",
    "        print('MSE SCORE Depth = '+str(mse_score_depth))\n",
    "        if mse_score_depth <= mse_score_feature:\n",
    "            mse_score_feature = mse_score_depth\n",
    "            feature_limit += 1\n",
    "            continue\n",
    "        else:\n",
    "            return [feature_limit, depth_limit]\n",
    "\n",
    "outcome = RFDepthSelection(train_X, train_y, validate_X, validate_y)\n",
    "print('Best Feature = '+str(outcome[0]))\n",
    "print(\"Best Depth = \"+str(outcome[1]))\n",
    "\n",
    "#K-fold\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "tree_clf = DecisionTreeRegressor(max_depth=outcome[1], max_features=outcome[0])\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    tree_clf.fit(Xs[train],y[train])\n",
    "    regression_score = tree_clf.score(Xs[test], y[test])\n",
    "    y_pred = tree_clf.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = tree_clf.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "print('The Avg. Accuracy of Decision Tree Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Decision Tree Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Decision Tree Regression Training is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nDecision Tree Regression End \\n')\n",
    "# Decision Tree Regression End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef230aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Regression Start \n",
      "\n",
      "TEST\n",
      "41.67398357660278\n",
      "TRAIN\n",
      "14.675967864572465\n",
      "\n",
      "3\n",
      "TEST\n",
      "20.20433682149335\n",
      "TRAIN\n",
      "19.70702783185494\n",
      "\n",
      "3\n",
      "TEST\n",
      "15.746851941750743\n",
      "TRAIN\n",
      "19.996453965601063\n",
      "\n",
      "3\n",
      "TEST\n",
      "22.79139379345222\n",
      "TRAIN\n",
      "19.369565285253636\n",
      "\n",
      "3\n",
      "TEST\n",
      "19.343575267124997\n",
      "TRAIN\n",
      "19.568827309127137\n",
      "\n",
      "3\n",
      "The Avg. Accuracy of Neural Network Regression is -1.11\n",
      "The Avg. Mean Square Error for Neural Network Regression Test is 23.9520\n",
      "The Avg. Mean Square Error for Neural Network Regression Training is 18.6636\n",
      "\n",
      "Neural Network Regression End \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nNeural Network Regression Start \\n')\n",
    "\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "\n",
    "def hyperparameter(trainx, trainy, validatex, validatey):\n",
    "    hyperparameterlimit = 1\n",
    "    bestlimit = 1\n",
    "    mse_score = 1000\n",
    "    while hyperparameterlimit <= 200:\n",
    "        print('Hyperparameter loop no. '+str(hyperparameterlimit))\n",
    "        mlp_clf = MLPRegressor(random_state=1, max_iter=400, \n",
    "                               hidden_layer_sizes = hyperparameterlimit).fit(trainx, trainy)\n",
    "        y_pred = mlp_clf.predict(validatex)\n",
    "        mse = mean_absolute_error(validatey, y_pred)\n",
    "        print(hyperparameterlimit)\n",
    "        print(mse)\n",
    "        if mse < mse_score:# - (mse_score/20):\n",
    "            mse_score = mse\n",
    "            bestlimit = hyperparameterlimit\n",
    "        hyperparameterlimit+=1\n",
    "    return hyperparameterlimit\n",
    "# Hidden Layer size = minimal difference to test - only effects training\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(x)\n",
    "#hyperparameterval = hyperparameter(train_X, train_y, validate_X, validate_y) \n",
    "#K-fold\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    mlp_clf = MLPRegressor(random_state=1, max_iter=8,\n",
    "                          activation = 'relu', solver = 'lbfgs', learning_rate = 'adaptive',\n",
    "                          hidden_layer_sizes = 5).fit(Xs[train], y[train])\n",
    "    regression_score = mlp_clf.score(Xs[test], y[test])\n",
    "    y_pred = mlp_clf.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = mlp_clf.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    print(mlp_clf.n_layers_)\n",
    "    score_total += regression_score\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "print('The Avg. Accuracy of Neural Network Regression is {:03.2f}'.format(score_total/5))\n",
    "print('The Avg. Mean Square Error for Neural Network Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Neural Network Regression Training is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nNeural Network Regression End \\n')\n",
    "# Neural Network Regression End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRandom Forest Regression Start \\n')\n",
    "\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "#Find best feature to accuracy ratio - random forest has built in feature selection via feature ranking\n",
    "# return Highest rated features\n",
    "def RFDepthSelection(trainx, trainy, testx, testy):\n",
    "    feature_limit = 1\n",
    "    mse_score_feature = 1000\n",
    "    mse_score_depth = 1000\n",
    "    while feature_limit <= 114:\n",
    "        #rint('Feature Loop '+str(feature_limit))\n",
    "        mse_score_depth = 1000\n",
    "        depth_limit = 1\n",
    "        while depth_limit <= 114:\n",
    "            #rint('Depth Loop '+str(depth_limit))\n",
    "            rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, criterion = 'absolute_error',\n",
    "                                                max_features = feature_limit, max_depth = depth_limit)\n",
    "            rfregressor.fit(trainx, trainy)\n",
    "            y_pred = rfregressor.predict(testx)\n",
    "            mse = mean_absolute_error(testy, y_pred)\n",
    "            if mse < mse_score_depth:# - (mse_score/20):\n",
    "                mse_score_depth = mse\n",
    "                depth_limit+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        #rint('CHECK MSE')\n",
    "        #rint('MSE = '+str(mse))\n",
    "        #rint('MSE_SCORE Feature = '+str(mse_score_feature))\n",
    "        #rint('MSE SCORE Depth = '+str(mse_score_depth))\n",
    "        if mse_score_depth <= mse_score_feature:\n",
    "            mse_score_feature = mse_score_depth\n",
    "            feature_limit += 1\n",
    "            continue\n",
    "        else:\n",
    "            return [feature_limit, depth_limit]\n",
    "\n",
    "validateoutcome = RFDepthSelection(train_X, train_y, validate_X, validate_y)\n",
    "feature_limit = validateoutcome[0]\n",
    "depth_limit = validateoutcome[1]\n",
    "rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, criterion = 'absolute_error',\n",
    "                                            max_features = feature_limit, max_depth = depth_limit)\n",
    "print(\"Feature Limit = \"+str(feature_limit))\n",
    "print(\"Depth Limit = \"+str(depth_limit))\n",
    "\n",
    "#Make x the validated feature\n",
    "#rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = feature_limit) #Default Measure = MSE\n",
    "#K-fold\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    rfregressor.fit(Xs[train], y[train])\n",
    "    y_pred = rfregressor.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = rfregressor.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "#Find best feature to accuracy ratio - random forest has built in feature selection via feature ranking\n",
    "# return Highest rated features\n",
    "def RFDepthSelection(trainx, trainy, testx, testy):\n",
    "    feature_limit = 1\n",
    "    mse_score_feature = 1000\n",
    "    mse_score_depth = 1000\n",
    "    while feature_limit <= 114:\n",
    "        print('Feature Loop '+str(feature_limit))\n",
    "        mse_score_depth = 1000\n",
    "        depth_limit = 1\n",
    "        while depth_limit <= 114:\n",
    "            print('Depth Loop '+str(depth_limit))\n",
    "            rfregressor = ExtraTreesRegressor(n_estimators=1000, random_state = 0, criterion = 'absolute_error',\n",
    "                                                max_features = feature_limit, max_depth = depth_limit)\n",
    "            rfregressor.fit(trainx, trainy)\n",
    "            y_pred = rfregressor.predict(testx)\n",
    "            mse = mean_absolute_error(testy, y_pred)\n",
    "            if mse < mse_score_depth:# - (mse_score/20):\n",
    "                mse_score_depth = mse\n",
    "                depth_limit+=1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        print('CHECK MSE')\n",
    "        print('MSE = '+str(mse))\n",
    "        print('MSE_SCORE Feature = '+str(mse_score_feature))\n",
    "        print('MSE SCORE Depth = '+str(mse_score_depth))\n",
    "        if mse_score_depth <= mse_score_feature:\n",
    "            mse_score_feature = mse_score_depth\n",
    "            feature_limit += 1\n",
    "            continue\n",
    "        else:\n",
    "            return [feature_limit, depth_limit]\n",
    "\n",
    "validateoutcome = RFDepthSelection(train_X, train_y, validate_X, validate_y)\n",
    "feature_limit = validateoutcome[0]\n",
    "depth_limit = validateoutcome[1]\n",
    "rfregressor = ExtraTreesRegressor(n_estimators=1000, random_state = 0, criterion = 'absolute_error',\n",
    "                                            max_features = feature_limit, max_depth = depth_limit)\n",
    "print(\"Feature Limit = \"+str(feature_limit))\n",
    "print(\"Depth Limit = \"+str(depth_limit))\n",
    "\n",
    "#Make x the validated feature\n",
    "#rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = feature_limit) #Default Measure = MSE\n",
    "#K-fold\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    rfregressor.fit(Xs[train], y[train])\n",
    "    y_pred = rfregressor.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = rfregressor.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f022dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "#Find best feature to accuracy ratio - random forest has built in feature selection via feature ranking\n",
    "# return Highest rated features\n",
    "def RFDepthSelection(trainx, trainy, testx, testy):\n",
    "    depth_limit = 1\n",
    "    mse_score_feature = 1000\n",
    "    mse_score_depth = 1000\n",
    "    while depth_limit <= 114:\n",
    "        print('Depth Loop '+str(depth_limit))\n",
    "        rfregressor = HistGradientBoostingRegressor(max_iter = 10, max_depth = depth_limit)\n",
    "        rfregressor.fit(trainx, trainy)\n",
    "        y_pred = rfregressor.predict(testx)\n",
    "        mse = mean_absolute_error(testy, y_pred)\n",
    "        if mse < mse_score_depth:# - (mse_score/20):\n",
    "            mse_score_depth = mse\n",
    "            depth_limit+=1\n",
    "            continue\n",
    "        else:\n",
    "            return depth_limit\n",
    "\n",
    "validateoutcome = RFDepthSelection(train_X, train_y, validate_X, validate_y)\n",
    "rfregressor = HistGradientBoostingRegressor(max_iter = 10, max_depth = validateoutcome)\n",
    "print(\"Depth Limit = \"+str(validateoutcome))\n",
    "\n",
    "#Make x the validated feature\n",
    "#rfregressor = RandomForestRegressor(n_estimators=100, random_state = 0, max_features = feature_limit) #Default Measure = MSE\n",
    "#K-fold\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(train_X)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, train_y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    rfregressor.fit(Xs[train], y[train])\n",
    "    y_pred = rfregressor.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = rfregressor.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Random Forest Regression Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "reg = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3, shuffle = False, penalty='elasticnet'))\n",
    "reg.fit(x,y)\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    reg.fit(Xs[train], y[train])\n",
    "    y_pred = reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for SGD Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for SGD Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "\n",
    "huber=HuberRegressor()\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    huber.fit(Xs[train], y[train])\n",
    "    y_pred = huber.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = huber.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Huber Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Huber Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "\n",
    "reg=RANSACRegressor()\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    reg.fit(Xs[train], y[train])\n",
    "    y_pred = reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for RANSAC Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for RANSAC Train is {:03.4f}'.format(mse2_total/5))\n",
    "print('\\nRandom Forest Regression End \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf44282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TheilSenRegressor\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "\n",
    "reg=TheilSenRegressor()\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    reg.fit(Xs[train], y[train])\n",
    "    y_pred = reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Theil Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Theil Train is {:03.4f}'.format(mse2_total/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "x = all_df.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(x, y, test_size=0.15, shuffle = False)\n",
    "\n",
    "reg=Ridge(alpha=1.0)\n",
    "Xs = scaler.fit_transform(x)\n",
    "mse_total = 0\n",
    "mse2_total = 0\n",
    "score_total = 0\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(Xs, y):\n",
    "    #featureno = RFDepthSelection(Xs[train], y[train], Xs[test], y[test]) #Use Nested Kfold for this?\n",
    "    #print(featureno)\n",
    "    #rfregressor = RandomForestRegressor(n_estimators=1000, random_state = 0, max_features = featureno)\n",
    "    reg.fit(Xs[train], y[train])\n",
    "    y_pred = reg.predict(Xs[test])\n",
    "    mse = mean_absolute_error(y[test], y_pred)\n",
    "    y_pred2 = reg.predict(Xs[train])\n",
    "    mse2 = mean_absolute_error(y[train], y_pred2)\n",
    "    print('TEST')\n",
    "    print(mse)\n",
    "    print('TRAIN')\n",
    "    print(mse2)\n",
    "    print('')\n",
    "    mse_total += mse\n",
    "    mse2_total += mse2\n",
    "#print(\"Best Number of Features = \"+str(feature_limit))\n",
    "print('The Avg. Mean Square Error for Ridge Test is {:03.4f}'.format(mse_total/5))\n",
    "print('The Avg. Mean Square Error for Ridge Train is {:03.4f}'.format(mse2_total/5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
